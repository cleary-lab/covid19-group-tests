## Read in extracted Wolfel data
viral_loads_dat <- read.csv("drosten_data.csv")
ggplot(viral_loads_dat) + geom_line(aes(x=t,y=obs)) + facet_wrap(~i)
viral_loads_dat <- viral_loads_dat[,c("t","obs","i")]
## Run MCMC
mcmcPars1 <- c("iterations"=200000,"popt"=0.44,"opt_freq"=1000,
"thin"=10,"adaptive_period"=100000,"save_block"=1000)
mcmcPars2 <- c("iterations"=200000,"popt"=0.234,"opt_freq"=1000,
"thin"=10,"adaptive_period"=100000,"save_block"=1000)
parTab[parTab$names == "viral_peak_sd", "values"] <- 0.9
parTab[parTab$names == "viral_peak_sd", "fixed"] <- 1
prior_func <- function(pars){
names(pars) <- parTab$names
to_test <- pars["viral_peak_sd"]
prior <- dnorm(to_test, 1, 0.25, TRUE)
prior
}
f <- create_func_indivs(parTab, viral_loads_dat,ver="posterior")
f(parTab$values)
chains <- load_mcmc_chains("~/Google Drive/nCoV/pool_samples/chains", parTab, FALSE, 1, burnin=200000)
chain <- read.csv(output$file)
chain <- chain[chain$sampno > mcmcPars2["adaptive_period"],]
plot(coda::as.mcmc(chain[chain$sampno > 50000,]))
plot(chains[[2]])
list.files()
getwd()
setwd("chains")
list.files()
res <- foreach(i=seq_along(filenames),.packages = "lazymcmc") %dopar% {
setwd("/Users/james/Google Drive/nCoV/pool_samples/chains")
startTab <- generate_start_tab(parTab)
## Generate random starting conditions for the chain
## Note that a list of starting tables is created,
## one for each temperature chain
x <- run_MCMC(startTab, viral_loads_dat, mcmcPars=mcmcPars1,filename=filenames[i],
CREATE_POSTERIOR_FUNC = create_func_indivs,PRIOR_FUNC=prior_func,ver="posterior",
OPT_TUNING=0.2)
## Check convergence
chain <- read.csv(x$file)
best_pars <- get_best_pars(chain)
chain <- chain[chain$sampno >= mcmcPars1["adaptive_period"],2:(ncol(chain)-1)]
covMat <- cov(chain)
mvrPars <- list(covMat,1,w=0.8)
## Start from best location of previous chain
startTab$values <- best_pars
## Run second chain
output <- run_MCMC(startTab, viral_loads_dat, mcmcPars=mcmcPars2,filename=filenames[i],
CREATE_POSTERIOR_FUNC = create_func_indivs,PRIOR_FUNC=prior_func,ver="posterior",
OPT_TUNING=0.2, mvrPars=mvrPars)
}
#devtools::install_github("jameshay218/lazymcmc")
library(lazymcmc)
library(tidyverse)
library(data.table)
library(coda)
library(MASS)
library(doParallel)
setwd("~/Google Drive/nCoV/pool_samples/")
source("model_funcs.R")
n_clusters <- 3
cl <- makeCluster(n_clusters)
registerDoParallel(cl)
## Choose working directory
#setwd("~/Documents/GitHub/covid19-group-tests/code/viral_kinetics/")
filenames <- paste0("drosten_data_", 1:3)
n_indiv <- 9
parTab <- read.csv("partab.csv",stringsAsFactors=FALSE)
## Read in extracted Wolfel data
viral_loads_dat <- read.csv("drosten_data.csv")
ggplot(viral_loads_dat) + geom_line(aes(x=t,y=obs)) + facet_wrap(~i)
viral_loads_dat <- viral_loads_dat[,c("t","obs","i")]
## Run MCMC
mcmcPars1 <- c("iterations"=200000,"popt"=0.44,"opt_freq"=1000,
"thin"=10,"adaptive_period"=100000,"save_block"=1000)
mcmcPars2 <- c("iterations"=200000,"popt"=0.234,"opt_freq"=1000,
"thin"=10,"adaptive_period"=100000,"save_block"=1000)
parTab[parTab$names == "viral_peak_sd", "values"] <- 0.9
parTab[parTab$names == "viral_peak_sd", "fixed"] <- 1
prior_func <- function(pars){
names(pars) <- parTab$names
to_test <- pars["viral_peak_sd"]
prior <- dnorm(to_test, 1, 0.25, TRUE)
prior
}
f <- create_func_indivs(parTab, viral_loads_dat,ver="posterior")
f(parTab$values)
res <- foreach(i=seq_along(filenames),.packages = "lazymcmc") %dopar% {
setwd("/Users/james/Google Drive/nCoV/pool_samples/chains")
startTab <- generate_start_tab(parTab)
## Generate random starting conditions for the chain
## Note that a list of starting tables is created,
## one for each temperature chain
x <- run_MCMC(startTab, viral_loads_dat, mcmcPars=mcmcPars1,filename=filenames[i],
CREATE_POSTERIOR_FUNC = create_func_indivs,PRIOR_FUNC=prior_func,ver="posterior",
OPT_TUNING=0.2)
## Check convergence
chain <- read.csv(x$file)
best_pars <- get_best_pars(chain)
chain <- chain[chain$sampno >= mcmcPars1["adaptive_period"],2:(ncol(chain)-1)]
covMat <- cov(chain)
mvrPars <- list(covMat,1,w=0.8)
## Start from best location of previous chain
startTab$values <- best_pars
## Run second chain
output <- run_MCMC(startTab, viral_loads_dat, mcmcPars=mcmcPars2,filename=filenames[i],
CREATE_POSTERIOR_FUNC = create_func_indivs,PRIOR_FUNC=prior_func,ver="posterior",
OPT_TUNING=0.2, mvrPars=mvrPars)
}
#devtools::install_github("jameshay218/lazymcmc")
library(lazymcmc)
library(tidyverse)
library(data.table)
library(coda)
library(MASS)
library(doParallel)
setwd("~/Google Drive/nCoV/pool_samples/")
source("model_funcs.R")
n_clusters <- 3
cl <- makeCluster(n_clusters)
registerDoParallel(cl)
## Choose working directory
#setwd("~/Documents/GitHub/covid19-group-tests/code/viral_kinetics/")
filenames <- paste0("drosten_data_", 1:3)
n_indiv <- 9
parTab <- read.csv("partab.csv",stringsAsFactors=FALSE)
## Read in extracted Wolfel data
viral_loads_dat <- read.csv("drosten_data.csv")
ggplot(viral_loads_dat) + geom_line(aes(x=t,y=obs)) + facet_wrap(~i)
viral_loads_dat <- viral_loads_dat[,c("t","obs","i")]
## Run MCMC
mcmcPars1 <- c("iterations"=200000,"popt"=0.44,"opt_freq"=1000,
"thin"=10,"adaptive_period"=100000,"save_block"=1000)
mcmcPars2 <- c("iterations"=200000,"popt"=0.234,"opt_freq"=1000,
"thin"=10,"adaptive_period"=100000,"save_block"=1000)
parTab[parTab$names == "viral_peak_sd", "values"] <- 0.9
parTab[parTab$names == "viral_peak_sd", "fixed"] <- 1
prior_func <- function(pars){
names(pars) <- parTab$names
to_test <- pars["viral_peak_sd"]
prior <- dnorm(to_test, 1, 0.25, TRUE)
prior
}
f <- create_func_indivs(parTab, viral_loads_dat,ver="posterior")
f(parTab$values)
res <- foreach(i=seq_along(filenames),.packages = "lazymcmc") %dopar% {
setwd("/Users/james/Google Drive/nCoV/pool_samples/chains")
startTab <- generate_start_tab(parTab)
## Generate random starting conditions for the chain
## Note that a list of starting tables is created,
## one for each temperature chain
x <- run_MCMC(startTab, viral_loads_dat, mcmcPars=mcmcPars1,filename=filenames[i],
CREATE_POSTERIOR_FUNC = create_func_indivs,PRIOR_FUNC=prior_func,ver="posterior",
OPT_TUNING=0.2)
## Check convergence
chain <- read.csv(x$file)
best_pars <- get_best_pars(chain)
chain <- chain[chain$sampno >= mcmcPars1["adaptive_period"],2:(ncol(chain)-1)]
covMat <- cov(chain)
mvrPars <- list(covMat,1,w=0.8)
## Start from best location of previous chain
startTab$values <- best_pars
## Run second chain
output <- run_MCMC(startTab, viral_loads_dat, mcmcPars=mcmcPars2,filename=filenames[i],
CREATE_POSTERIOR_FUNC = create_func_indivs,PRIOR_FUNC=prior_func,ver="posterior",
OPT_TUNING=0.2, mvrPars=mvrPars)
}
chains <- load_mcmc_chains("~/Google Drive/nCoV/pool_samples/chains", parTab, FALSE, 1, burnin=200000)
plot(chains[[2]])
chain <- chains[[2]]
chain$sampno <- 1:nrow(chain)
## Simulate posterior draws over 30 days
times <- seq(0,30,by=0.1)
dat_fake <- data.frame(t=rep(times, n_indiv),obs=0,i=rep(seq_len(n_indiv),each=length(times)))
## 1000 posterior draws
samps <- sample(unique(chain$sampno), 1000)
store_all <- matrix(nrow=1000,ncol=nrow(dat_fake))
store_all_obs <- matrix(nrow=1000,ncol=nrow(dat_fake))
f_model <- create_func_indivs(parTab, dat_fake, ver="model",remove_pre_tp = FALSE)
## Solve model for each draw
for(i in seq_along(samps)){
pars <- get_index_par(chain, samps[i])
pred <- f_model(pars)
## Also simulate observations (ie. draw Normal noise)
obs <- pred + rnorm(length(pred), 0, pars["sd"])
store_all[i,] <- pred
store_all_obs[i,] <- obs
}
head(chain)
class(chain)
chains <- load_mcmc_chains("~/Google Drive/nCoV/pool_samples/chains", parTab, TRUE, 1, burnin=200000)
plot(chains[[2]])
chains <- load_mcmc_chains("~/Google Drive/nCoV/pool_samples/chains", parTab, FALSE, 1, burnin=200000,multi=TRUE)
plot(chains[[2]])
chains <- load_mcmc_chains("~/Google Drive/nCoV/pool_samples/chains", parTab, FALSE, 1, burnin=200000,multi=TRUE)
plot(chains[[2]])
chain <- chains[[2]]
chain$sampno <- 1:nrow(chain)
chain <- as.data.frame(chains[[2]])
chain$sampno <- 1:nrow(chain)
## Simulate posterior draws over 30 days
times <- seq(0,30,by=0.1)
dat_fake <- data.frame(t=rep(times, n_indiv),obs=0,i=rep(seq_len(n_indiv),each=length(times)))
## 1000 posterior draws
samps <- sample(unique(chain$sampno), 1000)
store_all <- matrix(nrow=1000,ncol=nrow(dat_fake))
store_all_obs <- matrix(nrow=1000,ncol=nrow(dat_fake))
f_model <- create_func_indivs(parTab, dat_fake, ver="model",remove_pre_tp = FALSE)
## Solve model for each draw
for(i in seq_along(samps)){
pars <- get_index_par(chain, samps[i])
pred <- f_model(pars)
## Also simulate observations (ie. draw Normal noise)
obs <- pred + rnorm(length(pred), 0, pars["sd"])
store_all[i,] <- pred
store_all_obs[i,] <- obs
}
## Generate quantiles
quants <- t(apply(store_all,2, function(x) quantile(x,c(0.025,0.5,0.975),na.rm=TRUE)))
colnames(quants) <- c("lower","median","upper")
quants <- as_tibble(quants)
dat_fake <- as_tibble(dat_fake)
quants <- bind_cols(dat_fake, quants)
quants_obs <- t(apply(store_all_obs,2, function(x) quantile(x,c(0.025,0.5,0.975),na.rm=TRUE)))
colnames(quants_obs) <- c("lower","median","upper")
quants_obs <- as_tibble(quants_obs)
dat_fake <- as_tibble(dat_fake)
quants_obs <- bind_cols(dat_fake, quants_obs)
ggplot() +
geom_ribbon(data=quants_obs, aes(x=t,ymin=lower,ymax=upper),fill="blue",alpha=0.25) +
geom_ribbon(data=quants,aes(x=t,ymin=lower,ymax=upper),fill="blue",alpha=0.5) +
geom_line(data=quants, aes(x=t,y=median),col="blue") +
geom_point(data=viral_loads_dat,aes(x=t,y=obs)) +
coord_cartesian(ylim=c(0,10)) +
scale_y_continuous(breaks=seq(0,10,by=1)) +
geom_hline(yintercept=2,linetype="dashed") +
theme_bw() +
ylab("log10 RNA titre") +
xlab("Days post symptom onset") +
facet_wrap(~i)
simulate_epidemic_process <- function(n_indivs, growth_rate, times){
incidence <- exp(growth_rate*times)/n_indivs
incidence <- incidence - incidence[1]
incidence[length(incidence)] <- 0
p_not_infected <- prod(1-incidence)
print(paste0("Cumulative incidence to date: ", signif(1-p_not_infected,3)))
p1 <- ggplot(tibble(time=times[1:(length(times)-1)],incidence=incidence[1:(length(times)-1)])) +
ylab("Daily probability of infection") +
xlab("Time") +
theme_bw() +
geom_line(aes(x=time,y=incidence))
return(list(plot=p1, incidence=incidence))
}
simulate_epidemic_process(100000,0.047,0:100)
simulate_epidemic_process <- function(n_indivs, growth_rate, times){
incidence <- exp(growth_rate*times)/n_indivs
incidence <- incidence - incidence[1]
incidence[length(incidence)] <- 0
p_not_infected <- prod(1-incidence)
print(paste0("Cumulative incidence to date: ", signif(1-p_not_infected,3)))
p1 <- ggplot(tibble(time=times[1:(length(times)-1)],incidence=incidence[1:(length(times)-1)])) +
ylab("Daily probability of infection") +
xlab("Time") +
theme_bw() +
geom_line(aes(x=time,y=incidence))
return(list(plot=p1, incidence=incidence, overall_prob_infection=1-p_not_infected))
}
simulate_infection_times <- function(n, p_infected, incidence){
scaled_incidence <- incidence/sum(incidence)
are_infected <- numeric(n)
infection_times <- numeric(n)
for(i in 1:n){
infection <- rbinom(1,1 p_infected)
are_infected[i] <- infection
if(infection == 1){
t_inf <- sample(1:length(incidence), 1, prob=scaled_incidence)
infection_times[i] <- t_inf
} else {
infection_times[i] <- -1
}
}
return(infection_times)
}
source('~/Google Drive/nCoV/pool_samples/simulation_functions.R', echo=TRUE)
omg <- simulate_epidemic_process(100000, 0.047,0:100)
simulate_infection_times(1000, omg[["overall_prob_infection"]], omg[["incidence"]])
omg1 <- simulate_infection_times(1000, omg[["overall_prob_infection"]], omg[["incidence"]])
hist(omg1)
hist(omg1,breaks=101)
hist(omg1[omg1 > 0],breaks=101)
omg1 <- simulate_infection_times(10000, omg[["overall_prob_infection"]], omg[["incidence"]])
hist(omg1[omg1 > 0],breaks=101)
simulate_symptom_onsets <- function(infection_times, incubation_period_par1=1.57, incubation_period_par2=0.65){
onset_times <- numeric(length(infection_times))
for(i in seq_along(onset_times)){
if(infection_times[i] > 0){
tmp_onset_time <- rlnorm(1, incubation_period_par1, incubation_par2)
onset_times[i] <- infection_times[i] + tmp_onset_time
} else {
onset_times[i] <- -1
}
}
return(onset_times)
}
model_func
source('~/Google Drive/nCoV/pool_samples/simulation_functions.R', echo=TRUE)
source('~/Google Drive/nCoV/pool_samples/simulation_functions.R', echo=TRUE)
source('~/Google Drive/nCoV/pool_samples/simulation_functions.R', echo=TRUE)
epidemic_process <- simulate_epidemic_process(100000,0.047,0:100)
epidemic_process
infection_times <- simulate_infection_times(1000, epidemic_process$overall_prob_infection, epidemic_process$incidence)
onset_times <- simulate_symptom_onsets(infection_times)
simulate_symptom_onsets <- function(infection_times, incubation_period_par1=1.57, incubation_period_par2=0.65){
onset_times <- numeric(length(infection_times))
for(i in seq_along(onset_times)){
if(infection_times[i] > 0){
tmp_onset_time <- rlnorm(1, incubation_period_par1, incubation_period_par2)
onset_times[i] <- infection_times[i] + tmp_onset_time
} else {
onset_times[i] <- -1
}
}
return(onset_times)
}
onset_times <- simulate_symptom_onsets(infection_times)
onset_times
head(chain)
mean(chain$wane_mean)
mean(chain$wane_sd)
pars <- list(viral_peak_mean=7, viral_peak_sd=1,
wane_mean=0.6, wane_sd=0.2)
viral_loads <- simulate_viral_loads(infection_times, onset_times, 0:100, pars)
simulate_viral_loads <- function(infection_times, onset_times, times, kinetics_pars){
viral_peak_mean <- kinetics_pars$viral_peak_mean
viral_peak_sd <- kinetics_pars$viral_peak_sd
wane_mean <- kinetics_pars$wane_mean
wane_sd <- kinetics_pars$wane_sd
viral_loads <- matrix(0, nrow=length(infection_times), ncol=length(times))
for(i in seq_along(infection_times)){
if(infection_times[i] > 0){
viral_peak <- max(rnorm(1, viral_peak_mean, viral_peak_sd),0.01)
tp <- runif(1,0,7)
wane <- max(rnorm(1, wane_mean, wane_sd),0.001)
incubation_period <- onset_times[i] - infection_times[i]
viral_load <- model_func(times, tp+incubation_period, viral_peak, wane)
viral_load[viral_load < 0] <- 0
viral_loads[i,t_inf:ncol(viral_loads)] <- viral_load[seq_along(t_inf:ncol(viral_loads))]
}
}
return(viral_loads)
}
viral_loads <- simulate_viral_loads(infection_times, onset_times, 0:100, pars)
simulate_viral_loads <- function(infection_times, onset_times, times, kinetics_pars){
viral_peak_mean <- kinetics_pars$viral_peak_mean
viral_peak_sd <- kinetics_pars$viral_peak_sd
wane_mean <- kinetics_pars$wane_mean
wane_sd <- kinetics_pars$wane_sd
viral_loads <- matrix(0, nrow=length(infection_times), ncol=length(times))
for(i in seq_along(infection_times)){
if(infection_times[i] > 0){
viral_peak <- max(rnorm(1, viral_peak_mean, viral_peak_sd),0.01)
tp <- runif(1,0,7)
wane <- max(rnorm(1, wane_mean, wane_sd),0.001)
incubation_period <- onset_times[i] - infection_times[i]
viral_load <- model_func(times, tp+incubation_period, viral_peak, wane)
viral_load[viral_load < 0] <- 0
viral_loads[i,infection_times[i]:ncol(viral_loads)] <- viral_load[seq_along(infection_times[i]:ncol(viral_loads))]
}
}
return(viral_loads)
}
epidemic_process <- simulate_epidemic_process(100000,0.047,0:100)
infection_times <- simulate_infection_times(1000, epidemic_process$overall_prob_infection, epidemic_process$incidence)
onset_times <- simulate_symptom_onsets(infection_times)
pars <- list(viral_peak_mean=7, viral_peak_sd=1,
wane_mean=0.6, wane_sd=0.2)
viral_loads <- simulate_viral_loads(infection_times, onset_times, 0:100, pars)
image(viral_loads)
image(t(viral_loads))
rowSums(viral_loads)
plot(viral_loads[which(rowSums(viral_loads) > 0),][1,])
plot(viral_loads[which(rowSums(viral_loads) > 0),][2,])
plot(viral_loads[which(rowSums(viral_loads) > 0),][3,])
plot(viral_loads[which(rowSums(viral_loads) > 0),][4,])
plot(viral_loads[which(rowSums(viral_loads) > 0),][5,])
plot(viral_loads[which(rowSums(viral_loads) > 0),][6,])
plot(viral_loads[which(rowSums(viral_loads) > 0),][7,])
plot(viral_loads[which(rowSums(viral_loads) > 0),][8,])
plot(viral_loads[which(rowSums(viral_loads) > 0),][9,])
infection_times
viral_loads_bin <- viral_loads[viral_loads >= 2]
image(viral_loads_bin)
viral_loads_bin
viral_loads_bin <- apply(viral_loads, 1, function(x) x >= 2)
viral_loads_bin
image(viral_loads_bin)
image(t(viral_loads_bin))
image(viral_loads_bin)
epidemic_process <- simulate_epidemic_process(100000,0.1,0:100)
infection_times <- simulate_infection_times(1000, epidemic_process$overall_prob_infection, epidemic_process$incidence)
epidemic_process$plot
infection_times <- simulate_infection_times(1000, epidemic_process$overall_prob_infection, epidemic_process$incidence)
onset_times <- simulate_symptom_onsets(infection_times)
pars <- list(viral_peak_mean=7, viral_peak_sd=1,
wane_mean=0.6, wane_sd=0.2)
viral_loads <- simulate_viral_loads(infection_times, onset_times, 0:100, pars)
viral_loads_bin <- apply(viral_loads, 1, function(x) x >= 2)
image(viral_loads_bin)
epidemic_process <- simulate_epidemic_process(100000,0.05,0:100)
epidemic_process$plot
infection_times <- simulate_infection_times(1000, epidemic_process$overall_prob_infection, epidemic_process$incidence)
onset_times <- simulate_symptom_onsets(infection_times)
pars <- list(viral_peak_mean=7, viral_peak_sd=1,
wane_mean=0.6, wane_sd=0.2)
viral_loads <- simulate_viral_loads(infection_times, onset_times, 0:100, pars)
viral_loads_bin <- apply(viral_loads, 1, function(x) x >= 2)
image(viral_loads_bin)
library(tidyverse)
library(ggplot2)
source("simulation_functions.R")
epidemic_process <- simulate_epidemic_process(100000,0.05,0:100)
epidemic_process$plot
infection_times <- simulate_infection_times(1000, epidemic_process$overall_prob_infection, epidemic_process$incidence)
onset_times <- simulate_symptom_onsets(infection_times)
pars <- list(viral_peak_mean=7, viral_peak_sd=1,
wane_mean=0.6, wane_sd=0.2)
viral_loads <- simulate_viral_loads(infection_times, onset_times, 0:100, pars)
viral_loads_bin <- apply(viral_loads, 1, function(x) x >= 2)
image(viral_loads_bin)
viral_loads
## Viral kinetics pars
## Change wd to local path
parTab <- read.csv("partab.csv",stringsAsFactors=FALSE)
chains <- load_mcmc_chains("~/Google Drive/nCoV/pool_samples/chains", parTab, FALSE, 1, burnin=200000,multi=TRUE)
library(tidyverse)
library(ggplot2)
library(lazymcmc)
source("simulation_functions.R")
source("model_funcs.R")
## Entire popuation size
population_n <- 100000
## Sample size
n <- 1000
## Epidemic growth rate
growth_rate <- 0.05
## Duration of epidemic so far in days
times <- 0:100
## Viral kinetics pars
## Change wd to local path
parTab <- read.csv("partab.csv",stringsAsFactors=FALSE)
chains <- load_mcmc_chains("~/Google Drive/nCoV/pool_samples/chains", parTab, FALSE, 1, burnin=200000,multi=TRUE)
chain <- as.data.frame(chains[[2]])
pars <- list(viral_peak_mean=mean(chain$viral_peak_mean), viral_peak_sd=mean(chain$viral_peak_sd),
wane_mean=mean(chain$wane_mean), wane_sd=mean(chain$wane_sd))
epidemic_process <- simulate_epidemic_process(population_n,growth_rate,times)
epidemic_process$plot
infection_times <- simulate_infection_times(n, epidemic_process$overall_prob_infection, epidemic_process$incidence)
onset_times <- simulate_symptom_onsets(infection_times)
viral_loads <- simulate_viral_loads(infection_times, onset_times, 0:100, pars)
viral_loads_bin <- apply(viral_loads, 1, function(x) x >= 2)
image(viral_loads_bin)
plot(coda::as.mcmc(chain))
ggplot() +
geom_ribbon(data=quants_obs, aes(x=t,ymin=lower,ymax=upper),fill="blue",alpha=0.25) +
geom_ribbon(data=quants,aes(x=t,ymin=lower,ymax=upper),fill="blue",alpha=0.5) +
geom_line(data=quants, aes(x=t,y=median),col="blue") +
geom_point(data=viral_loads_dat,aes(x=t,y=obs)) +
coord_cartesian(ylim=c(0,10)) +
scale_y_continuous(breaks=seq(0,10,by=1)) +
geom_hline(yintercept=2,linetype="dashed") +
theme_bw() +
ylab("log10 RNA titre") +
xlab("Days post symptom onset") +
facet_wrap(~i)
source('~/Google Drive/nCoV/pool_samples/simulate_data.R', echo=TRUE)
## Simulate the epidemic process
epidemic_process <- simulate_epidemic_process(population_n,growth_rate,times)
## Simulate the epidemic process
epidemic_process <- simulate_epidemic_process(population_n,0.1,times)
source('~/Google Drive/nCoV/pool_samples/simulate_data.R', echo=TRUE)
chains <- load_mcmc_chains("~/Documents/GitHub/covid19-group-tests/code/viral_kinetics/chains", parTab, FALSE, 1, burnin=200000,multi=TRUE)
## Change to your local filepath
setwd("~/Documents/GitHub/covid19-group-tests/code/viral_kinetics/")
source("simulation_functions.R")
source("model_funcs.R")
## Entire popuation size (this is MA in 2018 from US Census Bureau)
population_n <- 6900000
## Sample size
n <- 1000
## Epidemic growth rate
growth_rate <- 0.1
## Duration of epidemic so far in days
times <- 0:100
## Viral kinetics pars
## Change wd to local path
parTab <- read.csv("pars/partab.csv",stringsAsFactors=FALSE)
chains <- load_mcmc_chains("~/Documents/GitHub/covid19-group-tests/code/viral_kinetics/chains", parTab, FALSE, 1, burnin=200000,multi=TRUE)
chain <- as.data.frame(chains[[2]])
pars <- list(viral_peak_mean=mean(chain$viral_peak_mean), viral_peak_sd=mean(chain$viral_peak_sd),
wane_mean=mean(chain$wane_mean), wane_sd=mean(chain$wane_sd),
tp_last_day=7)
## Simulate the epidemic process
epidemic_process <- simulate_epidemic_process(population_n,0.1,times)
epidemic_process$plot
## Simulate infection times
infection_times <- simulate_infection_times(n, epidemic_process$overall_prob_infection, epidemic_process$incidence)
## Simulate symptom onset times using default incubation period distribution
onset_times <- simulate_symptom_onsets(infection_times)
## Simulate viral loads for the sample population
viral_loads <- simulate_viral_loads(infection_times, onset_times, times, pars)
## Have a look to see the window of detection for each simulated individual
viral_loads_bin <- apply(viral_loads, 1, function(x) x >= 2)
image(viral_loads_bin)
